{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPLOYMENT MODEL PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>This notebook contains the pre-processing and model tuning for deployment. Since we will be deploying the model on Free Tier of AWS EC2 instance, we need to create the model and required tables which would require less compute power. Hence we will model on the top 300 features, taken from the best single XGBoost Models Feature Importances. We will optimize the LightGBM model and check the performance on Test Data, to make sure that the model doesn't perform too well. <br>\n",
    "We are making a sort of a trade-off between performance and compute requirements.</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:02:46.384961Z",
     "start_time": "2020-10-26T21:02:44.409231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing Useful DataStructures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3\n",
    "\n",
    "#importing Misc Libraries\n",
    "import gc\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "#sklearn libraries\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#bayesian optimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "#lightgbm\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#for 100% jupyter notebook cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T20:30:13.141913Z",
     "start_time": "2020-10-26T20:30:13.128719Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(data, verbose = True):\n",
    "    #source: https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "    '''\n",
    "    This function is used to reduce the memory usage by converting the datatypes of a pandas\n",
    "    DataFrame withing required limits.\n",
    "    '''\n",
    "    \n",
    "    start_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('-'*100)\n",
    "        print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "\n",
    "    end_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "        print('-'*100)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T20:55:27.448024Z",
     "start_time": "2020-10-26T20:55:27.160175Z"
    }
   },
   "outputs": [],
   "source": [
    "def relational_tables_prepare(table_name, file_directory = '', verbose = True, num_top_cols = 300):\n",
    "    '''\n",
    "    Function to pickle the relational tables which would need to be merged during production with the \n",
    "    test datapoint\n",
    "    \n",
    "    Inputs:\n",
    "        table_name: str\n",
    "            The name of file to be pickled.\n",
    "        file_directory: str, default = ''\n",
    "            The directory in which files are saved\n",
    "        verbose: bool, default = True\n",
    "            Whether to keep verbosity or not\n",
    "        num_top_cols: int, default = 300\n",
    "            Number of columns to keep out of 600 for deployment\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Loading the tables into memory...\")\n",
    "        start = datetime.now()\n",
    "        \n",
    "    #loading all the tables in memory, for dimensionality reduction\n",
    "    with open(file_directory + 'bureau_merged_preprocessed.pkl', 'rb') as f:\n",
    "        bureau_aggregated = reduce_mem_usage(pickle.load(f), verbose = False)\n",
    "    with open(file_directory + 'previous_application_preprocessed.pkl', 'rb') as f:\n",
    "        previous_aggregated = reduce_mem_usage(pickle.load(f), verbose = False)\n",
    "    with open(file_directory + 'installments_payments_preprocessed.pkl', 'rb') as f:\n",
    "        installments_aggregated = reduce_mem_usage(pickle.load(f), verbose = False)\n",
    "    with open(file_directory + 'POS_CASH_balance_preprocessed.pkl', 'rb') as f:\n",
    "        pos_aggregated = reduce_mem_usage(pickle.load(f), verbose = False)\n",
    "    with open(file_directory + 'credit_card_balance_preprocessed.pkl', 'rb') as f:\n",
    "        cc_aggregated = reduce_mem_usage(pickle.load(f), verbose = False)\n",
    "    with open(file_directory + 'application_train_preprocessed.pkl', 'rb') as f:\n",
    "        application_train = reduce_mem_usage(pickle.load(f), verbose = False) \n",
    "    with open(file_directory + 'application_test_preprocessed.pkl', 'rb') as f:\n",
    "        application_test = reduce_mem_usage(pickle.load(f), verbose = False)\n",
    "    #select only num_to_cols\n",
    "    with open('Final_XGBOOST_Selected_features.pkl', 'rb') as f:\n",
    "        final_cols = pickle.load(f)[:num_top_cols]\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Done.\")\n",
    "        print(f\"Time Elapsed = {datetime.now() - start}\")\n",
    "        start2 = datetime.now()\n",
    "        print(\"\\nRemoving the non-useful features...\")\n",
    "        \n",
    "    #removing non-useful columns from pre-processed previous_application table\n",
    "    previous_app_columns_to_keep = set(previous_aggregated.columns).intersection(set(final_cols)).union(\n",
    "                                    set([ele for ele in previous_aggregated.columns if 'AMT_ANNUITY' in ele] + [ele for ele in previous_aggregated.columns if 'AMT_GOODS' in ele]))\n",
    "    previous_aggregated = previous_aggregated[previous_app_columns_to_keep]\n",
    "    #removing non-useful columns from pre-processed credit_card_balance table\n",
    "    credit_card_balance_columns_to_keep = set(cc_aggregated.columns).intersection(set(final_cols)).union(\n",
    "                                    set([ele for ele in cc_aggregated.columns if 'AMT_RECEIVABLE_PRINCIPAL' in ele] + \n",
    "                                        [ele for ele in cc_aggregated.columns if 'AMT_RECIVABLE' in ele] + \n",
    "                                        [ele for ele in cc_aggregated.columns if 'TOTAL_RECEIVABLE' in ele] + ['SK_ID_CURR']))\n",
    "    cc_aggregated = cc_aggregated[credit_card_balance_columns_to_keep]\n",
    "    #removing non-useful columns from pre-processed installments_payments table\n",
    "    installments_payments_columns_to_keep = set(installments_aggregated.columns).intersection(set(final_cols)).union(\n",
    "                                            set([ele for ele in installments_aggregated.columns if 'AMT_PAYMENT' in \n",
    "                                                 ele and 'RATIO' not in ele and 'DIFF' not in ele] + ['AMT_INSTALMENT_MEAN_MAX', 'AMT_INSTALMENT_SUM_MAX']))\n",
    "    installments_aggregated = installments_aggregated[installments_payments_columns_to_keep]\n",
    "    #removing non-useful columns from pre-processed bureau-aggregated table\n",
    "    bureau_columns_to_keep =  set(bureau_aggregated.columns).intersection(set(final_cols)).union([ele for ele in bureau_aggregated.columns\n",
    "                                        if 'DAYS_CREDIT' in ele and 'ENDDATE' not in ele and 'UPDATE' not in ele] + [ele for ele in bureau_aggregated.columns if\n",
    "                                        'AMT_CREDIT' in ele and 'OVERDUE' in ele] + [ele for ele in bureau_aggregated.columns if 'AMT_ANNUITY' in ele and 'CREDIT'  not in ele])\n",
    "    bureau_aggregated = bureau_aggregated[bureau_columns_to_keep]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Done.\")\n",
    "        print(f\"Time Elapsed = {datetime.now() - start2}\")\n",
    "        print(\"\\nMerging all the tables, and saving to pickle file 'relational_table.pkl'...\")\n",
    "\n",
    "    #merging all the tables\n",
    "    relational_table = cc_aggregated.merge(bureau_aggregated, on = 'SK_ID_CURR', how = 'outer')\n",
    "    relational_table = relational_table.merge(previous_aggregated, on = 'SK_ID_CURR', how = 'outer')\n",
    "    relational_table = relational_table.merge(installments_aggregated, on = 'SK_ID_CURR', how = 'outer')\n",
    "    relational_table = relational_table.merge(pos_aggregated, on = 'SK_ID_CURR', how = 'outer')\n",
    "    relational_table = reduce_mem_usage(relational_table, verbose = False)\n",
    "\n",
    "    with open('LGBM Deployment/' + table_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(relational_table, f)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Done.\")\n",
    "        print(f\"Total Time taken = {datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling on 300 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data with top 300 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T20:33:26.646406Z",
     "start_time": "2020-10-26T20:33:25.216039Z"
    }
   },
   "outputs": [],
   "source": [
    "#loading the training and test data\n",
    "with open('train_data_final.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('test_data_final.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "    \n",
    "#getting the test SK_ID_CURR and train class labels\n",
    "target_train = train_data.pop('TARGET')\n",
    "skid_test = test_data.pop('SK_ID_CURR')\n",
    "#remvoing sk_id_curr from train data\n",
    "_ = train_data.pop('SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T20:33:53.650702Z",
     "start_time": "2020-10-26T20:33:52.342229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train Data = (307507, 300)\n",
      "Shape of Test Data = (48744, 300)\n"
     ]
    }
   ],
   "source": [
    "#loading the final columns for modelling, obtained from XGBoost\n",
    "#choosing only first 300 columns\n",
    "with open('Final_XGBOOST_Selected_features.pkl', 'rb') as f:\n",
    "    final_cols = pickle.load(f)[:300]\n",
    "train_data = train_data[final_cols]\n",
    "test_data = test_data[final_cols]\n",
    "print(f\"Shape of Train Data = {train_data.shape}\")\n",
    "print(f\"Shape of Test Data = {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization for LGBM Model (to be deployed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T20:35:29.146715Z",
     "start_time": "2020-10-26T20:35:29.135743Z"
    }
   },
   "outputs": [],
   "source": [
    "def lgbm_evaluation(num_leaves, max_depth, min_split_gain, min_child_weight,\n",
    "                    min_child_samples, subsample, colsample_bytree, reg_alpha, reg_lambda):\n",
    "    '''\n",
    "    Function for Bayesian Optimization of LightGBM's Hyperparamters. Takes the hyperparameters as input, and\n",
    "    returns the Cross-Validation AUC as output.\n",
    "    \n",
    "    Inputs: Hyperparamters to be tuned.\n",
    "        num_leaves, max_depth, min_split_gain, min_child_weight,\n",
    "        min_child_samples, subsample, colsample_bytree, reg_alpha, reg_lambda\n",
    "        \n",
    "    Returns:\n",
    "        CV ROC-AUC Score\n",
    "    '''\n",
    "    \n",
    "    params = {\n",
    "        'objective' : 'binary',\n",
    "        'boosting_type' : 'gbdt',\n",
    "        'learning_rate' : 0.05,\n",
    "        'n_estimators' : 5000,\n",
    "        'n_jobs' : -1,\n",
    "        'num_leaves' : int(round(num_leaves)),\n",
    "        'max_depth' : int(round(max_depth)),\n",
    "        'min_split_gain' : min_split_gain,\n",
    "        'min_child_weight' : min_child_weight,\n",
    "        'min_child_samples' : int(round(min_child_samples)),\n",
    "        'subsample': subsample,\n",
    "        'subsample_freq' : 1,\n",
    "        'colsample_bytree' : colsample_bytree,\n",
    "        'reg_alpha' : reg_alpha,\n",
    "        'reg_lambda' : reg_lambda,\n",
    "        'verbosity' : -1,\n",
    "        'seed' : 2131\n",
    "    }\n",
    "    \n",
    "    stratified_cv = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 312)\n",
    "    \n",
    "    cv_preds = np.zeros(train_data.shape[0])\n",
    "    for train_indices, cv_indices in stratified_cv.split(train_data, target_train):\n",
    "\n",
    "        x_tr = train_data.iloc[train_indices]\n",
    "        y_tr = target_train.iloc[train_indices]\n",
    "        x_cv = train_data.iloc[cv_indices]\n",
    "        y_cv = target_train.iloc[cv_indices]\n",
    "\n",
    "        lgbm_clf = lgb.LGBMClassifier(**params)\n",
    "        lgbm_clf.fit(x_tr, y_tr, eval_set= [(x_cv, y_cv)],\n",
    "                eval_metric='auc', verbose = False, early_stopping_rounds=200)\n",
    "\n",
    "        cv_preds[cv_indices] = lgbm_clf.predict_proba(x_cv, num_iteration = lgbm_clf.best_iteration_)[:,1]\n",
    "\n",
    "    return roc_auc_score(target_train, cv_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T20:55:27.150965Z",
     "start_time": "2020-10-26T20:35:38.418056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | max_depth | min_ch... | min_ch... | min_sp... | num_le... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8011  \u001b[0m | \u001b[0m 0.6084  \u001b[0m | \u001b[0m 8.094   \u001b[0m | \u001b[0m 36.07   \u001b[0m | \u001b[0m 17.77   \u001b[0m | \u001b[0m 0.08661 \u001b[0m | \u001b[0m 40.52   \u001b[0m | \u001b[0m 0.1314  \u001b[0m | \u001b[0m 0.2057  \u001b[0m | \u001b[0m 0.5302  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.802   \u001b[0m | \u001b[95m 0.9637  \u001b[0m | \u001b[95m 6.587   \u001b[0m | \u001b[95m 54.72   \u001b[0m | \u001b[95m 34.07   \u001b[0m | \u001b[95m 0.08797 \u001b[0m | \u001b[95m 32.6    \u001b[0m | \u001b[95m 0.1777  \u001b[0m | \u001b[95m 0.02717 \u001b[0m | \u001b[95m 0.5482  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8014  \u001b[0m | \u001b[0m 0.809   \u001b[0m | \u001b[0m 9.7     \u001b[0m | \u001b[0m 70.59   \u001b[0m | \u001b[0m 71.72   \u001b[0m | \u001b[0m 0.008772\u001b[0m | \u001b[0m 42.08   \u001b[0m | \u001b[0m 0.02745 \u001b[0m | \u001b[0m 0.2914  \u001b[0m | \u001b[0m 0.6052  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8017  \u001b[0m | \u001b[0m 0.8429  \u001b[0m | \u001b[0m 7.441   \u001b[0m | \u001b[0m 49.41   \u001b[0m | \u001b[0m 33.56   \u001b[0m | \u001b[0m 0.005236\u001b[0m | \u001b[0m 45.26   \u001b[0m | \u001b[0m 0.01503 \u001b[0m | \u001b[0m 0.01183 \u001b[0m | \u001b[0m 0.5295  \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.8027  \u001b[0m | \u001b[95m 0.8981  \u001b[0m | \u001b[95m 6.678   \u001b[0m | \u001b[95m 56.52   \u001b[0m | \u001b[95m 33.78   \u001b[0m | \u001b[95m 0.06873 \u001b[0m | \u001b[95m 31.95   \u001b[0m | \u001b[95m 0.2201  \u001b[0m | \u001b[95m 0.22    \u001b[0m | \u001b[95m 0.7734  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8018  \u001b[0m | \u001b[0m 0.6685  \u001b[0m | \u001b[0m 6.216   \u001b[0m | \u001b[0m 56.18   \u001b[0m | \u001b[0m 31.57   \u001b[0m | \u001b[0m 0.01131 \u001b[0m | \u001b[0m 31.62   \u001b[0m | \u001b[0m 0.2297  \u001b[0m | \u001b[0m 0.1069  \u001b[0m | \u001b[0m 0.5491  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8022  \u001b[0m | \u001b[0m 0.5145  \u001b[0m | \u001b[0m 6.251   \u001b[0m | \u001b[0m 57.29   \u001b[0m | \u001b[0m 33.16   \u001b[0m | \u001b[0m 0.03846 \u001b[0m | \u001b[0m 32.04   \u001b[0m | \u001b[0m 0.1139  \u001b[0m | \u001b[0m 0.08518 \u001b[0m | \u001b[0m 0.8994  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8021  \u001b[0m | \u001b[0m 0.5071  \u001b[0m | \u001b[0m 8.312   \u001b[0m | \u001b[0m 57.65   \u001b[0m | \u001b[0m 34.53   \u001b[0m | \u001b[0m 0.006578\u001b[0m | \u001b[0m 33.05   \u001b[0m | \u001b[0m 0.108   \u001b[0m | \u001b[0m 0.2601  \u001b[0m | \u001b[0m 0.7221  \u001b[0m |\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.8029  \u001b[0m | \u001b[95m 0.6729  \u001b[0m | \u001b[95m 8.466   \u001b[0m | \u001b[95m 55.99   \u001b[0m | \u001b[95m 33.97   \u001b[0m | \u001b[95m 0.02994 \u001b[0m | \u001b[95m 31.08   \u001b[0m | \u001b[95m 0.2498  \u001b[0m | \u001b[95m 0.04666 \u001b[0m | \u001b[95m 0.8487  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8022  \u001b[0m | \u001b[0m 0.6016  \u001b[0m | \u001b[0m 6.584   \u001b[0m | \u001b[0m 56.05   \u001b[0m | \u001b[0m 34.87   \u001b[0m | \u001b[0m 0.03665 \u001b[0m | \u001b[0m 29.61   \u001b[0m | \u001b[0m 0.2742  \u001b[0m | \u001b[0m 0.22    \u001b[0m | \u001b[0m 0.6093  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "bopt_lgbm_300 = BayesianOptimization(lgbm_evaluation, {'num_leaves' : (25,50),\n",
    "                                                   'max_depth' : (6,11),\n",
    "                                                   'min_split_gain' : (0, 0.1),\n",
    "                                                   'min_child_weight' : (5,80),\n",
    "                                                   'min_child_samples' : (5,80),\n",
    "                                                   'subsample' : (0.5,1),\n",
    "                                                   'colsample_bytree' : (0.5,1),\n",
    "                                                   'reg_alpha' : (0.001, 0.3),\n",
    "                                                   'reg_lambda' : (0.001, 0.3)},\n",
    "                                 random_state = 312)\n",
    "\n",
    "bayesian_optimization = bopt_lgbm_300.maximize(n_iter = 6, init_points = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T20:55:27.157748Z",
     "start_time": "2020-10-26T20:55:27.153193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters obtained for 300 selected features are:\n",
      "\n",
      "{'colsample_bytree': 0.6729420012253402, 'max_depth': 8.465800066653655, 'min_child_samples': 55.99234119409433, 'min_child_weight': 33.97047696286344, 'min_split_gain': 0.02993571494711166, 'num_leaves': 31.080485031008543, 'reg_alpha': 0.2498037855480203, 'reg_lambda': 0.04666011834689482, 'subsample': 0.8486579368120211}\n"
     ]
    }
   ],
   "source": [
    "#extracting the best parameters\n",
    "target_values = []\n",
    "for result in bopt_lgbm_300.res:\n",
    "    target_values.append(result['target'])\n",
    "    if result['target'] == max(target_values):\n",
    "        best_params = result['params']\n",
    "\n",
    "print(\"Best Hyperparameters obtained for 300 selected features are:\\n\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling on Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T20:57:34.135834Z",
     "start_time": "2020-10-26T20:55:27.450022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model on Tuned parameters:\n",
      "\n",
      "\tFold Number 1\n",
      "\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.804662\tvalid_0's binary_logloss: 0.231271\n",
      "[400]\tvalid_0's auc: 0.805389\tvalid_0's binary_logloss: 0.231017\n",
      "Early stopping, best iteration is:\n",
      "[279]\tvalid_0's auc: 0.805666\tvalid_0's binary_logloss: 0.230901\n",
      "\n",
      "\tFold Number 2\n",
      "\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.799072\tvalid_0's binary_logloss: 0.233269\n",
      "[400]\tvalid_0's auc: 0.800581\tvalid_0's binary_logloss: 0.232776\n",
      "[600]\tvalid_0's auc: 0.800355\tvalid_0's binary_logloss: 0.232935\n",
      "Early stopping, best iteration is:\n",
      "[415]\tvalid_0's auc: 0.800772\tvalid_0's binary_logloss: 0.232707\n",
      "\n",
      "\tFold Number 3\n",
      "\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.801353\tvalid_0's binary_logloss: 0.231669\n",
      "[400]\tvalid_0's auc: 0.802282\tvalid_0's binary_logloss: 0.231335\n",
      "Early stopping, best iteration is:\n",
      "[298]\tvalid_0's auc: 0.802581\tvalid_0's binary_logloss: 0.231236\n",
      "\n",
      "CV ROC-AUC Score = 0.8029447855365938\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        'objective' : 'binary',\n",
    "        'boosting_type' : 'gbdt',\n",
    "        'learning_rate' : 0.05,\n",
    "        'n_estimators' : 5000,\n",
    "        'n_jobs' : -1,\n",
    "        'num_leaves' : 31,\n",
    "        'max_depth' : 8,\n",
    "        'min_split_gain' : 0.02993571494711166,\n",
    "        'min_child_weight' : 33.97047696286344,\n",
    "        'min_child_samples' : 56,\n",
    "        'subsample': 0.8486579368120211,\n",
    "        'subsample_freq' : 1,\n",
    "        'colsample_bytree' : 0.6729420012253402,\n",
    "        'reg_alpha' : 0.2498037855480203,\n",
    "        'reg_lambda' : 0.04666011834689482,\n",
    "        'verbosity' : -1,\n",
    "        'seed' : 2131\n",
    "    }\n",
    "print(\"Fitting the model on Tuned parameters:\")\n",
    "#3 fold Stratified Cross Validation\n",
    "stratified_cv = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 312)\n",
    "\n",
    "#test and OOF-CV preds\n",
    "test_preds = np.zeros(test_data.shape[0])\n",
    "cv_preds = np.zeros(train_data.shape[0])\n",
    "\n",
    "for i, (train_indices, cv_indices) in enumerate(stratified_cv.split(train_data, target_train),1):\n",
    "    \n",
    "    print(f\"\\n\\tFold Number {i}\\n\")\n",
    "    x_tr = train_data.iloc[train_indices]\n",
    "    y_tr = target_train.iloc[train_indices]\n",
    "    x_cv = train_data.iloc[cv_indices]\n",
    "    y_cv = target_train.iloc[cv_indices]\n",
    "\n",
    "    lgbm_clf = lgb.LGBMClassifier(**params)\n",
    "    lgbm_clf.fit(x_tr, y_tr, eval_set= [(x_cv, y_cv)],\n",
    "            eval_metric='auc', verbose = 200, early_stopping_rounds=200)\n",
    "\n",
    "    cv_preds[cv_indices] = lgbm_clf.predict_proba(x_cv, num_iteration = lgbm_clf.best_iteration_)[:,1]\n",
    "    test_preds += lgbm_clf.predict_proba(test_data, num_iteration = lgbm_clf.best_iteration_)[:,1] /3\n",
    "    \n",
    "    #saving each folds model\n",
    "    with open(f'LGBM Deployment/clf_fold{i}.pkl', 'wb') as f:\n",
    "        pickle.dump(lgbm_clf, f)\n",
    "        \n",
    "#checking the Final ROC_AUC Score on CV Data\n",
    "print(f\"\\nCV ROC-AUC Score = {roc_auc_score(target_train, cv_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T19:10:24.628922Z",
     "start_time": "2020-10-26T19:10:24.625316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to Home Credit Default Risk\n"
     ]
    }
   ],
   "source": [
    "#submitting the result\n",
    "pd.DataFrame({'SK_ID_CURR':skid_test, 'TARGET' : test_preds}).to_csv('LGBM_deployment.csv', index = False)\n",
    "!kaggle competitions submit -c home-credit-default-risk -f LGBM_deployment.csv -m \"LightGBM model for deployment\"\n",
    "print('Successfully submitted to Home Credit Default Risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'LGBM Deployment Model.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T20:59:22.474286Z",
     "start_time": "2020-10-26T20:59:22.401482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold = 0.07180553151892075\n"
     ]
    }
   ],
   "source": [
    "#tuning the threshold for best J-Statistic\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(target_train, cv_preds)\n",
    "j_stat = tpr - fpr\n",
    "best_threshold = threshold[np.argmax(j_stat)]\n",
    "print(f\"Best Threshold = {best_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Relational Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving In Pickle Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:02:04.009649Z",
     "start_time": "2020-10-26T20:59:25.645391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the tables into memory...\n",
      "Done.\n",
      "Time Elapsed = 0:02:21.028309\n",
      "\n",
      "Removing the non-useful features...\n",
      "Done.\n",
      "Time Elapsed = 0:00:00.772336\n",
      "\n",
      "Merging all the tables, and saving to pickle file 'relational_table.pkl'...\n",
      "Done.\n",
      "Total Time taken = 0:02:38.289286\n"
     ]
    }
   ],
   "source": [
    "#saving the relational tables with reduced feature set\n",
    "relational_tables_prepare('relational_300_feats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving All Data in database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>For deployment, we would be needing the applications from application_train and application_test tables, for testing purpose. Because getting 120 inputs from User would be too time consuming, so we will only test the applications from these tables. So we will save them to the DataBase.\n",
    "We will create a DataBase named HOME_CREDIT_DB which will contain all the tables required during deployed phase.\n",
    "The tables stored will be:\n",
    "1. applications table\n",
    "2. relational table\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:13:05.300392Z",
     "start_time": "2020-10-26T21:13:05.292414Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_to_db(verbose = True):\n",
    "    '''\n",
    "    Function to save the required tables to DataBase\n",
    "    \n",
    "    Inputs:\n",
    "        verbose: bool, default = True\n",
    "            Whether to keep verbostiy or not\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Loading the files and saving to DataBase...\\n\")\n",
    "        start = datetime.now()\n",
    "        \n",
    "    #loading the application tables\n",
    "    application_train = reduce_mem_usage(pd.read_csv('application_train.csv'), verbose = False)\n",
    "    application_test = reduce_mem_usage(pd.read_csv('application_test.csv'), verbose = False)\n",
    "    #removing target column from application_train\n",
    "    _ = application_train.pop('TARGET')\n",
    "    #combining the train and test DataFrames\n",
    "    applications_all = application_train.append(application_test, ignore_index = True)\n",
    "    #saving this to sqlite database\n",
    "\n",
    "    print(\"Saving applications table to DataBase...\")\n",
    "    try:\n",
    "        #creating the DataBase\n",
    "        engine = create_engine('sqlite:///HOME_CREDIT_DB.db')\n",
    "        conn = engine.connect()\n",
    "        table_name = 'applications'\n",
    "        applications_all.to_sql(table_name, conn, index = False)\n",
    "        conn.close()\n",
    "\n",
    "        #also saving the relational table to this db\n",
    "        with open('LGBM Deployment/relational_300_feats.pkl', 'rb') as f:\n",
    "            relational_table = reduce_mem_usage(pickle.load(f), verbose = False)\n",
    "\n",
    "        conn = sqlite3.connect('HOME_CREDIT_DB.db')\n",
    "        table_name = 'relational_table'\n",
    "        relational_table.to_sql(table_name, conn, index = False)\n",
    "    finally:\n",
    "        conn.close()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Done.\")\n",
    "        print(f\"Time Taken = {datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:15:20.631867Z",
     "start_time": "2020-10-26T21:13:06.143444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the files and saving to DataBase...\n",
      "\n",
      "Saving applications table to DataBase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2612: UserWarning: The spaces in these column names will not be changed. In pandas versions < 0.14, spaces were converted to underscores.\n",
      "  method=method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Time Taken = 0:02:14.429377\n"
     ]
    }
   ],
   "source": [
    "save_to_db()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
